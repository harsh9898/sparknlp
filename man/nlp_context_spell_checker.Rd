% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/context-spell-checker.R
\name{nlp_context_spell_checker}
\alias{nlp_context_spell_checker}
\title{Spark NLP ContextSpellCheckerApproach}
\usage{
nlp_context_spell_checker(
  x,
  input_cols,
  output_col,
  batch_size = NULL,
  compound_count = NULL,
  case_strategy = NULL,
  class_count = NULL,
  epochs = NULL,
  error_threshold = NULL,
  final_learning_rate = NULL,
  initial_learning_rate = NULL,
  lm_classes = NULL,
  lazy_annotator = NULL,
  max_candidates = NULL,
  max_window_len = NULL,
  min_count = NULL,
  tradeoff = NULL,
  validation_fraction = NULL,
  weighted_dist_path = NULL,
  word_max_dist = NULL,
  uid = random_string("context_spell_checker_")
)
}
\arguments{
\item{x}{A \code{spark_connection}, \code{ml_pipeline}, or a \code{tbl_spark}.}

\item{input_cols}{Input columns. String array.}

\item{output_col}{Output column. String.}

\item{batch_size}{batch size for training in NLM. Defaults to 24}

\item{compound_count}{blacklist}

\item{case_strategy}{What case combinations to try when generating candidates. ALL_UPPER_CASE = 0,
FIRST_LETTER_CAPITALIZED = 1, ALL = 2. Defaults to 2.}

\item{class_count}{class threshold}

\item{epochs}{Number of epochs to train the language model. Defaults to 2.}

\item{error_threshold}{Threshold perplexity for a word to be considered as an error. Defaults to 10f.}

\item{final_learning_rate}{Final learning rate for the LM. Defaults to 0.0005}

\item{initial_learning_rate}{Initial learning rate for the LM. Defaults to 0.7}

\item{lm_classes}{Number of classes to use during factorization of the softmax output in the LM. Defaults to 2000.}

\item{lazy_annotator}{lazy annotator}

\item{max_candidates}{Maximum number of candidates for every word. Defaults to 6.}

\item{max_window_len}{Maximum size for the window used to remember history prior to every correction. Defaults to 5.}

\item{min_count}{Min number of times a token should appear to be included in vocab. Defaults to 3.0f.}

\item{tradeoff}{Tradeoff between the cost of a word error and a transition in the language model. Defaults to 18.0f.}

\item{validation_fraction}{Percentage of datapoints to use for validation. Defaults to .1f.}

\item{weighted_dist_path}{The path to the file containing the weighted_dist_path for the levenshtein distance.}

\item{word_max_dist}{Maximum distance for the generated candidates for every word. Defaults to 3.}

\item{uid}{A character string used to uniquely identify the ML estimator.}
}
\value{
The object returned depends on the class of \code{x}.

\itemize{
\item \code{spark_connection}: When \code{x} is a \code{spark_connection}, the function returns an instance of a \code{ml_estimator} object. The object contains a pointer to
a Spark \code{Estimator} object and can be used to compose
\code{Pipeline} objects.

\item \code{ml_pipeline}: When \code{x} is a \code{ml_pipeline}, the function returns a \code{ml_pipeline} with
the NLP estimator appended to the pipeline.

\item \code{tbl_spark}: When \code{x} is a \code{tbl_spark}, an estimator is constructed then
immediately fit with the input \code{tbl_spark}, returning an NLP model.
}
}
\description{
Spark ML estimator that Implements Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining
context information and word information
See \url{https://nlp.johnsnowlabs.com/docs/en/annotators#context-spellchecker}
}
