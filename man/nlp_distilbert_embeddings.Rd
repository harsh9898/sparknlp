% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/distilbert-embeddings.R
\name{nlp_distilbert_embeddings}
\alias{nlp_distilbert_embeddings}
\title{Spark NLP DistilBertEmbeddings}
\usage{
nlp_distilbert_embeddings(
  x,
  input_cols,
  output_col,
  batch_size = NULL,
  case_sensitive = NULL,
  dimension = NULL,
  input_annotator_types = NULL,
  max_sentence_length = NULL,
  output_annotator_type = NULL,
  storage_ref = NULL,
  uid = random_string("distilbert_embeddings_")
)
}
\arguments{
\item{x}{A \code{spark_connection}, \code{ml_pipeline}, or a \code{tbl_spark}.}

\item{input_cols}{Input columns. String array.}

\item{output_col}{Output column. String.}

\item{batch_size}{Size of every batch (Default depends on model).}

\item{case_sensitive}{Whether to ignore case in index lookups (Default depends on model)}

\item{dimension}{Number of embedding dimensions (Default depends on model)}

\item{input_annotator_types}{Input Annotator Types: DOCUMENT.}

\item{max_sentence_length}{Max sentence length to process (Default: 128)}

\item{output_annotator_type}{Output Annotator Types: WORD_EMBEDDINGS}

\item{storage_ref}{Unique identifier for storage (Default: this.uid)}

\item{uid}{A character string used to uniquely identify the ML estimator.}
}
\value{
The object returned depends on the class of \code{x}.

\itemize{
\item \code{spark_connection}: When \code{x} is a \code{spark_connection}, the function returns an instance of a \code{ml_estimator} object. The object contains a pointer to
a Spark \code{Estimator} object and can be used to compose
\code{Pipeline} objects.

\item \code{ml_pipeline}: When \code{x} is a \code{ml_pipeline}, the function returns a \code{ml_pipeline} with
the NLP estimator appended to the pipeline.

\item \code{tbl_spark}: When \code{x} is a \code{tbl_spark}, an estimator is constructed then
immediately fit with the input \code{tbl_spark}, returning an NLP model.
}
}
\description{
DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base.
It has 40\% less parameters than bert-base-uncased, runs 60\% faster while preserving over 95\% of
BERT's performances as measured on the GLUE language understanding benchmark.
See \url{https://nlp.johnsnowlabs.com/docs/en/transformers#distilbertembeddings}
}
