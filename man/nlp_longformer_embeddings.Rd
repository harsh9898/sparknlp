% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/longformer-embeddings.R
\name{nlp_longformer_embeddings}
\alias{nlp_longformer_embeddings}
\title{Spark NLP LongformerEmbeddings}
\usage{
nlp_longformer_embeddings(
  x,
  input_cols,
  output_col,
  batch_size = NULL,
  case_sensitive = NULL,
  dimension = NULL,
  max_sentence_length = NULL,
  storage_ref = NULL,
  uid = random_string("longformer_embeddings_")
)
}
\arguments{
\item{x}{A \code{spark_connection}, \code{ml_pipeline}, or a \code{tbl_spark}.}

\item{input_cols}{Input columns. String array.}

\item{output_col}{Output column. String.}

\item{batch_size}{Size of every batch (Default depends on model).}

\item{case_sensitive}{Whether to ignore case in index lookups (Default depends on model)}

\item{dimension}{Number of embedding dimensions (Default depends on model)}

\item{max_sentence_length}{Max sentence length to process (Default: 128)}

\item{storage_ref}{Unique identifier for storage (Default: this.uid)}

\item{uid}{A character string used to uniquely identify the ML estimator.}
}
\value{
The object returned depends on the class of \code{x}.

\itemize{
\item \code{spark_connection}: When \code{x} is a \code{spark_connection}, the function returns an instance of a \code{ml_estimator} object. The object contains a pointer to
a Spark \code{Estimator} object and can be used to compose
\code{Pipeline} objects.

\item \code{ml_pipeline}: When \code{x} is a \code{ml_pipeline}, the function returns a \code{ml_pipeline} with
the NLP estimator appended to the pipeline.

\item \code{tbl_spark}: When \code{x} is a \code{tbl_spark}, an estimator is constructed then
immediately fit with the input \code{tbl_spark}, returning an NLP model.
}
}
\description{
Longformer is a transformer model for long documents. The Longformer model was presented in Longformer:
The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan. longformer-base-4096
is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents.
It supports sequences of length up to 4,096.
See \url{https://nlp.johnsnowlabs.com/docs/en/transformers#longformerembeddings}
}
