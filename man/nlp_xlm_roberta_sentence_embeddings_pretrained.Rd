% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xlm_roberta_sentence_embeddings.R
\name{nlp_xlm_roberta_sentence_embeddings_pretrained}
\alias{nlp_xlm_roberta_sentence_embeddings_pretrained}
\title{Load a pretrained Spark NLP XlmRoBertaSentenceEmbeddings model}
\usage{
nlp_xlm_roberta_sentence_embeddings_pretrained(
  sc,
  input_cols,
  output_col,
  case_sensitive = NULL,
  batch_size = NULL,
  dimension = NULL,
  max_sentence_length = NULL,
  name = NULL,
  lang = NULL,
  remote_loc = NULL
)
}
\arguments{
\item{sc}{A Spark connection}

\item{input_cols}{Input columns. String array.}

\item{output_col}{Output column. String.}

\item{case_sensitive}{whether to lowercase tokens or not}

\item{batch_size}{batch size}

\item{dimension}{defines the output layer of BERT when calculating embeddings}

\item{max_sentence_length}{max sentence length to process}

\item{name}{the name of the model to load. If NULL will use the default value}

\item{lang}{the language of the model to be loaded. If NULL will use the default value}

\item{remote_loc}{the remote location of the model. If NULL will use the default value}
}
\value{
The Spark NLP model with the pretrained model loaded
}
\description{
Create a pretrained Spark NLP \code{XlmRoBertaSentenceEmbeddings} model.
See \url{https://nlp.johnsnowlabs.com/docs/en/annotators#xlmrobertasentenceembeddings}
}
\details{
Sentence-level embeddings using XLM-RoBERTa. The XLM-RoBERTa model was proposed in
Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau,
Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n,
Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on
Facebook's RoBERTa model released in 2019. It is a large multi-lingual language model,
trained on 2.5TB of filtered CommonCrawl data.
}
